{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uut4CBZMCqOh"
      },
      "source": [
        "# Final Exam Assignment (40 points - Total of 45 is Possible)\n",
        "## Due December 7, 2022, @ 8:00 am\n",
        "Note that there will be no extensions given for this assignment as there is a tight timeline for grading. \n",
        "\n",
        "For this assignment, I have provided each of you with your own training dataset. Your goal is to train a deep neural network to uncover the code image provided to you. \n",
        "\n",
        "I will provide you with instructions throughout. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "siMiNR67CqOo"
      },
      "outputs": [],
      "source": [
        "# Add your import statements here\n",
        "import os\n",
        "import numpy as np\n",
        "import urllib\n",
        "import time\n",
        "import sys\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "ydvrsEtTCqOq"
      },
      "outputs": [],
      "source": [
        "# This is a tool I have provided you to help you download your file.\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"\n",
        "    A function that downloads the data file from a URL\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : string\n",
        "        url where the file to download is located\n",
        "    filename : string\n",
        "        location where to save the file\n",
        "    reporthook : function\n",
        "        callback to display the download progress\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename, reporthook)\n",
        "        \n",
        "def reporthook(count, block_size, total_size):\n",
        "    \"\"\"\n",
        "    A function that displays the status and speed of the download\n",
        "    \"\"\"\n",
        "\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = int(progress_size / (1024 * duration + 0.0001))\n",
        "    percent = int(count * block_size * 100 / total_size)\n",
        "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
        "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "VjdyWeDnCqOr"
      },
      "outputs": [],
      "source": [
        "# You can download your file by typing your first name into the name block\n",
        "# The name used is the first part of your first name as listed in BB learn\n",
        "# If you have problems downloading the data please reach out to me\n",
        "\n",
        "name = 'Lauren'\n",
        "download_file(f'https://zenodo.org/record/7339649/files/data_{name}.npz?download=1','data.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXWvENmMCqOr"
      },
      "source": [
        "## Loading the Data (3 points)\n",
        "The data is provided to you as a compressed NumPy array saved as 'data.npz'. When working with real data you might need to figure out how data is stored. Use the information on 'npz' files to figure out what data you have. The data file contains three NumPy arrays. \n",
        "1. The features for the training dataset\n",
        "2. The regression values for the training dataset\n",
        "3. The validation features that contain your code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYVWbCYPJdX8",
        "outputId": "9f7e58bb-859b-443a-b20e-d4b7895e7e10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive, change directory to access file\n",
        "!cd \"/content/drive/MyDrive/t680\""
      ],
      "metadata": {
        "id": "wnuzzZVgJXgr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5sbNxYGtCqOs"
      },
      "outputs": [],
      "source": [
        "# Your Code goes here\n",
        "#os.listdir(\".\")\n",
        "#data = np.load('data.npz', allow_pickle = True)\n",
        "#lst = data.files\n",
        "#data = load_npz(\"data.npz\")\n",
        "#with np.load('data.npz', allow_pickle=True) as data:\n",
        "#    training_feat = data['training_feat']\n",
        "\n",
        "with np.load('data.npz', allow_pickle=True) as data:\n",
        "    training_feat = data['training_feat']   #features for training dataset\n",
        "    training_true = data['training_true']   #regression values for training dataset\n",
        "    validation_feat = data['validation_feat'] #validation features that contain your code\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UExFzmBSCqOt"
      },
      "source": [
        "## Preprocessing the Data (5 points)\n",
        "\n",
        "You should explore the data and figure out the best way to preprocess the data. \n",
        "\n",
        "Hints: \n",
        "1. For the regression values, these at the end will represent colors in RGB space from [0,1]. It is recommended to use a max-min scalar between 0 and 1. \n",
        "2. For the training features, you should look at the data and determine the best scaling method. Look at our class notes for a reminder of what other scaler might be useful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r2RVPch_CqOv"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "#1. For regression values, use max-min scalar between 0 and 1\n",
        "mm_scaler = MinMaxScaler()\n",
        "scaled_training_true = mm_scaler.fit_transform(training_true)   \n",
        "\n",
        "#2. For training features, use standard scaler\n",
        "s_scaler =  StandardScaler()\n",
        "scaled_training_feat = s_scaler.fit_transform(training_feat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQkXGOPiCqOv"
      },
      "source": [
        "## Building the Dataset (5 points)\n",
        "\n",
        "When training neural networks it is important to build a dataset that allows the machinery to sample the data. This also can be used to conduct some preprocessing of the data to make it work with PyTorch. \n",
        "\n",
        "I have provided you with the framework for a Dataset Class. \n",
        "\n",
        "You should:\n",
        "1. Convert the x and y data to a tensor 'float32' and put it on the GPU.\n",
        "2. Save the len of the data\n",
        "3. Add the code so when `__getitem__` is called it returns the x and y values\n",
        "3. make it so `__len__` returns the lenght when calle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "4SYPdiNlCqOw"
      },
      "outputs": [],
      "source": [
        "class Data(): #TODO: what is Dataset inheritance in the copied code\n",
        "  '''Dataset Class to store the samples and their corresponding labels, \n",
        "  and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X: np.ndarray, y: np.ndarray, device = 'cuda') -> None:\n",
        "\n",
        "    # need to convert float64 to float32 else \n",
        "    # will get the following error\n",
        "    # RuntimeError: expected scalar type Double but found Float\n",
        "    self.X = X.astype(np.float32)\n",
        "    self.y = y.astype(np.float32)\n",
        "    self.len = len(self.X)\n",
        "  \n",
        "  def __getitem__(self, index: int) -> tuple:\n",
        "    return self.X[index], self.y[index]\n",
        "    #TODO: why is the index needed here? not explained in the problem statement\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlYs7nmPCqOw"
      },
      "source": [
        "## Train-test Split (3 points)\n",
        "\n",
        "1. You should conduct a train-test split of the training data so you can make sure that your model does not overfit the data. A good ratio is 66/33 train \n",
        "2. You should instantiate the training dataset using the data class implemented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PBPg5KbGCqOw"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(scaled_training_feat, scaled_training_true, test_size = 0.33, random_state = 42)\n",
        "training_data = Data(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu4SIHHUCqOx"
      },
      "source": [
        "## Build the Dataloader (3 points)\n",
        "\n",
        "Pytorch uses DataLoaders to efficiently sample from a training dataset. Instantiate a Pytorch DataLoader using the dataset. \n",
        "\n",
        "You should set the following parameters:\n",
        "1. Batch size = 64\n",
        "2. Shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P6n1vlLSCqOx"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G62_UOgGCqOx"
      },
      "source": [
        "## Building a Neural Network (5 points)\n",
        "\n",
        "Using the provided class framework which inherits the `nn.Module` type in PyTorch builds a 4-layer neural network to complete the multiple regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "8WJzqUHiCqOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be22eba-3fac-44a4-e6d6-57a89c900ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class Neural_Network(nn.Module):\n",
        "  ''' Regression Model\n",
        "  ''' \n",
        "\n",
        "  # note, you can ignore the `:int` and `-> None` this is just more advanced doctring syntax\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
        "      '''The network has 4 layers\n",
        "            - input layer\n",
        "            - ReLu\n",
        "            - hidden layer\n",
        "            - ReLu\n",
        "            - hidden layer\n",
        "            - ReLu\n",
        "            - output layer\n",
        "      '''\n",
        "      super(Neural_Network, self).__init__()\n",
        "      # in this part you should intantiate each of the layer components\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(input_dim, input_dim),  #input\n",
        "          nn.ReLU(),                        #ReLu\n",
        "          nn.Linear(input_dim, hidden_dim), #hidden\n",
        "          nn.ReLU(),                        #ReLu\n",
        "          nn.Linear(hidden_dim, output_dim),#hidden\n",
        "          nn.ReLU(),                        #ReLu\n",
        "          nn.Linear(output_dim, output_dim))#output\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      # In this part you should build a model that returns the 3 outputs of the regression\n",
        "      # Type your code here\n",
        "      x = self.flatten(x)\n",
        "      x = self.linear_relu_stack(x)\n",
        "      #TODO: double check, class code uses a different variable to be returned\n",
        "      \n",
        "      return x\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me91Q395CqOy"
      },
      "source": [
        "## Instantiate the Model (3 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "OjLry29RCqOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46e43d7-3c83-40ef-8858-7d5028daf56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 5 3\n"
          ]
        }
      ],
      "source": [
        "# number of features (len of X cols)\n",
        "input_dim = X_train.shape[1]\n",
        "# number of hidden layers set this to 50\n",
        "hidden_layers = 5\n",
        "# Add the number of output dimensions\n",
        "output_dim = 3\n",
        "\n",
        "print(input_dim, hidden_layers, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "n0FZbo0jCqOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbbef8c-f009-4499-8a8d-b2753241795a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural_Network(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=30, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=5, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=5, out_features=3, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=3, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# initiate the regression model\n",
        "# make sure to put it on your GPU\n",
        "model = Neural_Network(input_dim, hidden_layers, output_dim).to(device)\n",
        "print(model)\n",
        "\n",
        "# criterion to computes the loss between input and target\n",
        "# Choose a good criteria\n",
        "loss_fn = nn.MSELoss() #for regression tasks\n",
        "# optimizer that will be used to update weights and biases\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-5)\n",
        "# you can choose any optimizer. I would recommend ADAM.\n",
        "# This problem should not be hard to optimize. A good starting learning rate is 3e-5. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82lwXNKdCqOz"
      },
      "source": [
        "## Train the Model (5 points)\n",
        "\n",
        "Training the model is conducted in a number of steps using loops.\n",
        "\n",
        "1. Set up a loop for each epoch\n",
        "2. Set a parameter to save the running loss\n",
        "3. Set up a nested loop that goes through the batches from the DataLoader you built\n",
        "    - I would recommend using enumerate to include the counts in the loop\n",
        "    - The dataloader will return a tuple that is the inputs and the labels\n",
        "4. Conduct the forward propagation of the model\n",
        "    - Give the model the inputs and compute the outputs\n",
        "    - Compute the loss given the criteria. \n",
        "5. Use the zero gradient method to remove the gradients from the optimizer\n",
        "6. Use the backward method to compute the gradients\n",
        "7. Use the step method in the optimizer to take an optimization step\n",
        "8. Compute the running loss by calling the item method and adding it to the running loss for each minibatch\n",
        "9. For each epoch print the epoch and the loss\n",
        "\n",
        "Note: If you find this challenging I would recommend that you look at examples of other pytorch training loops online. This is a very standard workflow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [
          "skip-execution"
        ],
        "id": "kJLESBbQCqOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f4796c-d0dc-4e68-a5f0-9814e8367fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.154061  [    0/67000]\n",
            "loss: 0.161866  [ 6400/67000]\n",
            "loss: 0.169725  [12800/67000]\n",
            "loss: 0.151751  [19200/67000]\n",
            "loss: 0.156710  [25600/67000]\n",
            "loss: 0.142009  [32000/67000]\n",
            "loss: 0.146157  [38400/67000]\n",
            "loss: 0.141429  [44800/67000]\n",
            "loss: 0.135038  [51200/67000]\n",
            "loss: 0.144667  [57600/67000]\n",
            "loss: 0.137341  [64000/67000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.152244  [    0/67000]\n",
            "loss: 0.145796  [ 6400/67000]\n",
            "loss: 0.138221  [12800/67000]\n",
            "loss: 0.132646  [19200/67000]\n",
            "loss: 0.145027  [25600/67000]\n",
            "loss: 0.130101  [32000/67000]\n",
            "loss: 0.132970  [38400/67000]\n",
            "loss: 0.135087  [44800/67000]\n",
            "loss: 0.132133  [51200/67000]\n",
            "loss: 0.124323  [57600/67000]\n",
            "loss: 0.122313  [64000/67000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.115280  [    0/67000]\n",
            "loss: 0.124893  [ 6400/67000]\n",
            "loss: 0.123447  [12800/67000]\n",
            "loss: 0.130655  [19200/67000]\n",
            "loss: 0.110241  [25600/67000]\n",
            "loss: 0.117155  [32000/67000]\n",
            "loss: 0.123664  [38400/67000]\n",
            "loss: 0.093301  [44800/67000]\n",
            "loss: 0.107020  [51200/67000]\n",
            "loss: 0.101698  [57600/67000]\n",
            "loss: 0.100147  [64000/67000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.099730  [    0/67000]\n",
            "loss: 0.103427  [ 6400/67000]\n",
            "loss: 0.099652  [12800/67000]\n",
            "loss: 0.100714  [19200/67000]\n",
            "loss: 0.097182  [25600/67000]\n",
            "loss: 0.102451  [32000/67000]\n",
            "loss: 0.093859  [38400/67000]\n",
            "loss: 0.110491  [44800/67000]\n",
            "loss: 0.101388  [51200/67000]\n",
            "loss: 0.099909  [57600/67000]\n",
            "loss: 0.098752  [64000/67000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.098525  [    0/67000]\n",
            "loss: 0.089421  [ 6400/67000]\n",
            "loss: 0.087606  [12800/67000]\n",
            "loss: 0.082914  [19200/67000]\n",
            "loss: 0.086616  [25600/67000]\n",
            "loss: 0.090144  [32000/67000]\n",
            "loss: 0.089754  [38400/67000]\n",
            "loss: 0.085033  [44800/67000]\n",
            "loss: 0.086212  [51200/67000]\n",
            "loss: 0.079116  [57600/67000]\n",
            "loss: 0.079275  [64000/67000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.072810  [    0/67000]\n",
            "loss: 0.076837  [ 6400/67000]\n",
            "loss: 0.075581  [12800/67000]\n",
            "loss: 0.085988  [19200/67000]\n",
            "loss: 0.083072  [25600/67000]\n",
            "loss: 0.072262  [32000/67000]\n",
            "loss: 0.073132  [38400/67000]\n",
            "loss: 0.079445  [44800/67000]\n",
            "loss: 0.074019  [51200/67000]\n",
            "loss: 0.069676  [57600/67000]\n",
            "loss: 0.077882  [64000/67000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.071764  [    0/67000]\n",
            "loss: 0.070107  [ 6400/67000]\n",
            "loss: 0.075925  [12800/67000]\n",
            "loss: 0.067988  [19200/67000]\n",
            "loss: 0.071589  [25600/67000]\n",
            "loss: 0.072226  [32000/67000]\n",
            "loss: 0.064259  [38400/67000]\n",
            "loss: 0.070949  [44800/67000]\n",
            "loss: 0.064547  [51200/67000]\n",
            "loss: 0.064952  [57600/67000]\n",
            "loss: 0.050585  [64000/67000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.059264  [    0/67000]\n",
            "loss: 0.055333  [ 6400/67000]\n",
            "loss: 0.052944  [12800/67000]\n",
            "loss: 0.061962  [19200/67000]\n",
            "loss: 0.060010  [25600/67000]\n",
            "loss: 0.059656  [32000/67000]\n",
            "loss: 0.057296  [38400/67000]\n",
            "loss: 0.063891  [44800/67000]\n",
            "loss: 0.064833  [51200/67000]\n",
            "loss: 0.053911  [57600/67000]\n",
            "loss: 0.045755  [64000/67000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.059261  [    0/67000]\n",
            "loss: 0.054041  [ 6400/67000]\n",
            "loss: 0.052332  [12800/67000]\n",
            "loss: 0.052005  [19200/67000]\n",
            "loss: 0.048444  [25600/67000]\n",
            "loss: 0.049826  [32000/67000]\n",
            "loss: 0.047235  [38400/67000]\n",
            "loss: 0.052868  [44800/67000]\n",
            "loss: 0.051159  [51200/67000]\n",
            "loss: 0.043383  [57600/67000]\n",
            "loss: 0.050358  [64000/67000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.047183  [    0/67000]\n",
            "loss: 0.051091  [ 6400/67000]\n",
            "loss: 0.041878  [12800/67000]\n",
            "loss: 0.049043  [19200/67000]\n",
            "loss: 0.040037  [25600/67000]\n",
            "loss: 0.046269  [32000/67000]\n",
            "loss: 0.035500  [38400/67000]\n",
            "loss: 0.045545  [44800/67000]\n",
            "loss: 0.040485  [51200/67000]\n",
            "loss: 0.034935  [57600/67000]\n",
            "loss: 0.044130  [64000/67000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.042163  [    0/67000]\n",
            "loss: 0.040032  [ 6400/67000]\n",
            "loss: 0.042446  [12800/67000]\n",
            "loss: 0.037545  [19200/67000]\n",
            "loss: 0.040673  [25600/67000]\n",
            "loss: 0.040941  [32000/67000]\n",
            "loss: 0.026901  [38400/67000]\n",
            "loss: 0.039752  [44800/67000]\n",
            "loss: 0.038880  [51200/67000]\n",
            "loss: 0.033900  [57600/67000]\n",
            "loss: 0.034459  [64000/67000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.034131  [    0/67000]\n",
            "loss: 0.031902  [ 6400/67000]\n",
            "loss: 0.034029  [12800/67000]\n",
            "loss: 0.031410  [19200/67000]\n",
            "loss: 0.036468  [25600/67000]\n",
            "loss: 0.032945  [32000/67000]\n",
            "loss: 0.034286  [38400/67000]\n",
            "loss: 0.038562  [44800/67000]\n",
            "loss: 0.029737  [51200/67000]\n",
            "loss: 0.029834  [57600/67000]\n",
            "loss: 0.033556  [64000/67000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.033701  [    0/67000]\n",
            "loss: 0.033304  [ 6400/67000]\n",
            "loss: 0.030182  [12800/67000]\n",
            "loss: 0.029297  [19200/67000]\n",
            "loss: 0.028077  [25600/67000]\n",
            "loss: 0.027401  [32000/67000]\n",
            "loss: 0.031911  [38400/67000]\n",
            "loss: 0.027576  [44800/67000]\n",
            "loss: 0.030952  [51200/67000]\n",
            "loss: 0.027081  [57600/67000]\n",
            "loss: 0.034503  [64000/67000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.034375  [    0/67000]\n",
            "loss: 0.027480  [ 6400/67000]\n",
            "loss: 0.022874  [12800/67000]\n",
            "loss: 0.023968  [19200/67000]\n",
            "loss: 0.031752  [25600/67000]\n",
            "loss: 0.029207  [32000/67000]\n",
            "loss: 0.027791  [38400/67000]\n",
            "loss: 0.023101  [44800/67000]\n",
            "loss: 0.026740  [51200/67000]\n",
            "loss: 0.026916  [57600/67000]\n",
            "loss: 0.023670  [64000/67000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.027638  [    0/67000]\n",
            "loss: 0.030147  [ 6400/67000]\n",
            "loss: 0.024690  [12800/67000]\n",
            "loss: 0.026659  [19200/67000]\n",
            "loss: 0.026023  [25600/67000]\n",
            "loss: 0.026202  [32000/67000]\n",
            "loss: 0.025691  [38400/67000]\n",
            "loss: 0.023722  [44800/67000]\n",
            "loss: 0.022718  [51200/67000]\n",
            "loss: 0.024736  [57600/67000]\n",
            "loss: 0.024246  [64000/67000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.023266  [    0/67000]\n",
            "loss: 0.024111  [ 6400/67000]\n",
            "loss: 0.021807  [12800/67000]\n",
            "loss: 0.025863  [19200/67000]\n",
            "loss: 0.021042  [25600/67000]\n",
            "loss: 0.022231  [32000/67000]\n",
            "loss: 0.024848  [38400/67000]\n",
            "loss: 0.019474  [44800/67000]\n",
            "loss: 0.018519  [51200/67000]\n",
            "loss: 0.021001  [57600/67000]\n",
            "loss: 0.021158  [64000/67000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.017765  [    0/67000]\n",
            "loss: 0.023053  [ 6400/67000]\n",
            "loss: 0.021603  [12800/67000]\n",
            "loss: 0.021695  [19200/67000]\n",
            "loss: 0.022929  [25600/67000]\n",
            "loss: 0.021060  [32000/67000]\n",
            "loss: 0.017758  [38400/67000]\n",
            "loss: 0.020144  [44800/67000]\n",
            "loss: 0.020797  [51200/67000]\n",
            "loss: 0.019444  [57600/67000]\n",
            "loss: 0.022167  [64000/67000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.018893  [    0/67000]\n",
            "loss: 0.018619  [ 6400/67000]\n",
            "loss: 0.019272  [12800/67000]\n",
            "loss: 0.018013  [19200/67000]\n",
            "loss: 0.017305  [25600/67000]\n",
            "loss: 0.016392  [32000/67000]\n",
            "loss: 0.019056  [38400/67000]\n",
            "loss: 0.017776  [44800/67000]\n",
            "loss: 0.018216  [51200/67000]\n",
            "loss: 0.017120  [57600/67000]\n",
            "loss: 0.016447  [64000/67000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.022058  [    0/67000]\n",
            "loss: 0.019208  [ 6400/67000]\n",
            "loss: 0.021873  [12800/67000]\n",
            "loss: 0.018293  [19200/67000]\n",
            "loss: 0.018353  [25600/67000]\n",
            "loss: 0.023780  [32000/67000]\n",
            "loss: 0.017393  [38400/67000]\n",
            "loss: 0.020235  [44800/67000]\n",
            "loss: 0.017411  [51200/67000]\n",
            "loss: 0.016056  [57600/67000]\n",
            "loss: 0.018626  [64000/67000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.017272  [    0/67000]\n",
            "loss: 0.019113  [ 6400/67000]\n",
            "loss: 0.018634  [12800/67000]\n",
            "loss: 0.014224  [19200/67000]\n",
            "loss: 0.015990  [25600/67000]\n",
            "loss: 0.015881  [32000/67000]\n",
            "loss: 0.016461  [38400/67000]\n",
            "loss: 0.016565  [44800/67000]\n",
            "loss: 0.013586  [51200/67000]\n",
            "loss: 0.016676  [57600/67000]\n",
            "loss: 0.011419  [64000/67000]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "epochs = 20 # sets the number of epochs to train 20 should be sufficent.\n",
        "# This should take about 5-10 minutes to train.\n",
        "\n",
        "# Your code should go here \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.cuda())\n",
        "        loss = loss_fn(pred, y.cuda())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    #test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrClvXXGCqOz"
      },
      "source": [
        "## Validate the Model (3 points)\n",
        "\n",
        "Use the test dataset from the train-test split to make sure your model is not overfitting\n",
        "\n",
        "1. You can build a dataloader as you did before, this time with the test data.\n",
        "2. Build a validation loop, which you should use `with torch.no_grad()` to make sure you do not modify the gradients, or weights. This will fix your model. \n",
        "3. Instantiate the loss to be 0.\n",
        "4. Build a similar loop to grab the validation dataset. \n",
        "5. Compute the predictions with the model.\n",
        "6. Compute the loss using your loss criteria.\n",
        "7. Print the final loss determined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZsfVUCp9CqO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0e3b9e-1cd9-4f60-ed77-9588020f2032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "0.015340705456342115\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "0.015339477482259042\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "0.015339783519188794\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "0.015340931673873534\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "0.015341206767791233\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "0.015338117901105867\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "0.015342100012628722\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "0.01534041668136799\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "0.015339488744764596\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "0.015339770868723823\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "0.015338295298202555\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "0.015340391777513562\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "0.015341373167701008\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "0.015342370673739749\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "0.01533929541591526\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "0.015344009875471508\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "0.015340789589424466\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "0.015340012754494137\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "0.01534124170682689\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "0.015340383271073071\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "test_data = Data(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.cuda())\n",
        "            test_loss += loss_fn(pred, y.cuda()).item()\n",
        "            \n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(test_loss)\n",
        "    \n",
        "    \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf0LggFoCqO0"
      },
      "source": [
        "<p style=\"color:blue\"> Question: Is your model overfitting or not? How do you know? (3 points) </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UnyrVvvCqO0"
      },
      "source": [
        "The loss during the testing loop seems to be slightly decreasing with each epoch, and is resting at about 0.0153. This is relatively close to the losses seen in the training regimine. Therefore, I do not think the model is currently overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr_6OnntCqO0"
      },
      "source": [
        "## Crack Your Code (3 points)\n",
        "\n",
        "1. You can build a dataloader as you did before, this time with the validation features to view your code.\n",
        "2. Build a loop, you should use `with torch.no_grad()` to make sure you do not modify the gradients or weights. This will fix your model. \n",
        "3. Compute the predictions of your model. \n",
        "    - Make sure you do all the same preprocess, the data has the same datatype, and is on the same device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94opWiqdCqO1"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tna_0IyUCqO1"
      },
      "source": [
        "## Reveal Your Code (3 points)\n",
        "\n",
        "Your code is an image. there are (65536, 3) predictions this corresponds to a (256,256,3) RGB image. \n",
        "1. Use the detach() method to remove the gradients from the tensor\n",
        "2. Transfer the tensor back to the 'cpu' if you had it on a GPU\n",
        "3. Reshape the image into a 256,256,3 array. \n",
        "4. Plot your successful result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deTTerlNCqO1"
      },
      "outputs": [],
      "source": [
        "# Your Code goes here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "c475b5beda6d617ffb7b2fcf453fbe132321ffc1e1f96c06cf49356e1e7f42cb"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}